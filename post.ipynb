{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over the past year or so, I've been using [JAX](https://jax.readthedocs.io) extensively for my research, and I've also been encouraging other astronomers to give it a try.\n",
    "In particular, I've been using JAX as the computation engine for probabilistic inference tasks.\n",
    "There's more to it, but one way that I like to think about JAX is as NumPy with just-in-time compilation and [automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation).\n",
    "The just-in-time compilation features of JAX can be used to speed up you NumPy computations by removing some Python overhead and by executing it on your GPU.\n",
    "Then, automatic differentiation can be used to efficiently compute the derivatives of your code with respect to its input parameters.\n",
    "These derivatives can substantially improve the performance of numerical inference methods (like maximum likelihood or Markov chain Monte Carlo) and for other tasks such as Fisher information analysis.\n",
    "\n",
    "This post isn't meant to be a comprehensive introduction to JAX (take a look at [the excellent JAX docs](https://jax.readthedocs.io/en/latest/notebooks/quickstart.html) for more of that) or to automatic differentiation ([I've written some words](https://docs.exoplanet.codes/en/latest/tutorials/autodiff/) about that, and [so have many others](https://www.google.com/search?q=automatic+differentiation)), but rather an introduction to the JAX ecosystem for probabilistic inference, with some examples that will be familiar to astronomers.\n",
    "From my perspective, one benefit of the JAX ecosystem compared to other similar tools available in Python (e.g. [PyMC](https://www.pymc.io), [Stan](https://mc-stan.org), etc.) is that it's generally more modular.\n",
    "In practice, this means that you can (relatively) easily combine different JAX libraries to develop your preferred workflow.\n",
    "For example, you can build a probabilistic model using [NumPyro](https://num.pyro.ai) that uses [tinygp](https://tinygp.readthedocs.io) for Gaussian Processes, and then run a Markov chain Monte Carlo (MCMC) analysis using [BlackJAX](https://github.com/blackjax-devs/blackjax).\n",
    "\n",
    "In this post, however, I'll focus primarily on providing an introduction to [NumPyro](https://num.pyro.ai), which is a probabilistic programming library that provides an interface for defining probabilistic models and running inference algorithms.\n",
    "At this point, NumPyro is probably the most mature JAX-based probabilistic programming library, and [its documentation page](https://num.pyro.ai) has a lot of examples, but I've found that these docs are not that user-friendly for my collaborators, so I wanted to provide a different perspective.\n",
    "In the following sections, I'll present two examples:\n",
    "\n",
    "1. The first example is a fairly simple linear regression problem that introduces some basic NumPyro concepts. In the second half of this example, we will re-implement the model from [my \"Mixture Models\" post](https://dfm.io/posts/mixture-models/) to account for outliers in the simulated dataset, while also introducing some more advanced elements.\n",
    "\n",
    "2. The second example is an astronomy-specific problem that is designed to really highlight the power of these methods. In this example, we will measure the distance to the [M67 open cluster](https://en.wikipedia.org/wiki/Messier_67) using a huge hierarchical model for the observed Gaia parallaxes of stars in the direction of M67. This example includes running an MCMC sampler with thousands of parameters, which would be intractable with the tools commonly used by astronomers, but only takes a few minutes to run using NumPyro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# We'll choose the parameters of our synthetic data.\n",
    "# The outlier probability will be 80%:\n",
    "true_frac = 0.8\n",
    "\n",
    "# The linear model has unit slope and zero intercept:\n",
    "true_params = [1.0, 0.0]\n",
    "\n",
    "# The outliers are drawn from a Gaussian with zero mean and unit variance:\n",
    "true_outliers = [0.0, 1.0]\n",
    "\n",
    "# For reproducibility, let's set the random number seed and generate the data:\n",
    "np.random.seed(12)\n",
    "x = np.sort(np.random.uniform(-2, 2, 15))\n",
    "yerr = 0.2 * np.ones_like(x)\n",
    "y = true_params[0] * x + true_params[1] + yerr * np.random.randn(len(x))\n",
    "\n",
    "# Those points are all drawn from the correct model so let's replace some of\n",
    "# them with outliers.\n",
    "m_bkg = np.random.rand(len(x)) > true_frac\n",
    "y[m_bkg] = true_outliers[0]\n",
    "y[m_bkg] += np.sqrt(true_outliers[1] + yerr[m_bkg] ** 2) * np.random.randn(sum(m_bkg))\n",
    "\n",
    "# Then save the *true* line.\n",
    "x0 = np.linspace(-2.1, 2.1, 200)\n",
    "y0 = np.dot(np.vander(x0, 2), true_params)\n",
    "\n",
    "# Plot the data and the truth.\n",
    "plt.errorbar(x, y, yerr=yerr, fmt=\",k\", ms=0, capsize=0, lw=1, zorder=999)\n",
    "plt.scatter(x[m_bkg], y[m_bkg], marker=\"s\", s=22, c=\"w\", edgecolor=\"k\", zorder=1000)\n",
    "plt.scatter(x[~m_bkg], y[~m_bkg], marker=\"o\", s=22, c=\"k\", zorder=1000)\n",
    "plt.plot(x0, y0, color=\"k\", lw=1.5)\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$y$\")\n",
    "plt.ylim(-2.5, 2.5)\n",
    "plt.xlim(-2.1, 2.1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import numpyro\n",
    "from numpyro import distributions as dist, infer\n",
    "\n",
    "numpyro.set_host_device_count(2)\n",
    "\n",
    "\n",
    "def linear_model(x, yerr, y=None):\n",
    "    # These are the parameters that we're fitting and we're required to define explicit\n",
    "    # priors using distributions from the numpyro.distributions module.\n",
    "    theta = numpyro.sample(\"theta\", dist.Uniform(-0.5 * jnp.pi, 0.5 * jnp.pi))\n",
    "    b_perp = numpyro.sample(\"b_perp\", dist.Normal(0, 1))\n",
    "\n",
    "    # Transformed parameters (and other things!) can be tracked during sampling using\n",
    "    # \"deterministics\" as follows:\n",
    "    m = numpyro.deterministic(\"m\", jnp.tan(theta))\n",
    "    b = numpyro.deterministic(\"b\", b_perp / jnp.cos(theta))\n",
    "\n",
    "    # Then we specify the sampling distribution for the data, or the likelihood function.\n",
    "    # Here we're using a numpyro.plate to indicate that the data are independent. This\n",
    "    # isn't actually necessary here and we could have equivalently omitted the plate since\n",
    "    # the Normal distribution can already handle vector-valued inputs. But, it's good to\n",
    "    # get into the habit of using plates because some inference algorithms or distributions\n",
    "    # can take advantage of knowing this structure.\n",
    "    with numpyro.plate(\"data\", len(x)):\n",
    "        numpyro.sample(\"y\", dist.Normal(m * x + b, yerr), obs=y)\n",
    "\n",
    "\n",
    "sampler = infer.MCMC(\n",
    "    infer.NUTS(linear_model),\n",
    "    num_warmup=2000,\n",
    "    num_samples=2000,\n",
    "    num_chains=2,\n",
    "    progress_bar=True,\n",
    ")\n",
    "%time sampler.run(jax.random.PRNGKey(0), x, yerr, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import corner\n",
    "import arviz as az\n",
    "\n",
    "inf_data = az.from_numpyro(sampler)\n",
    "corner.corner(inf_data, var_names=[\"m\", \"b\"], truths=true_params)\n",
    "az.summary(inf_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpyro_ext.distributions import MixtureGeneral\n",
    "\n",
    "\n",
    "def linear_mixture_model(x, yerr, y=None):\n",
    "    # Foreground model\n",
    "    theta = numpyro.sample(\"theta\", dist.Uniform(-0.5 * jnp.pi, 0.5 * jnp.pi))\n",
    "    b_perp = numpyro.sample(\"b_perp\", dist.Normal(0.0, 1.0))\n",
    "    m = numpyro.deterministic(\"m\", jnp.tan(theta))\n",
    "    b = numpyro.deterministic(\"b\", b_perp / jnp.cos(theta))\n",
    "    fg_dist = dist.Normal(m * x + b, yerr)\n",
    "\n",
    "    # Background model\n",
    "    bg_mean = numpyro.sample(\"bg_mean\", dist.Normal(0.0, 1.0))\n",
    "    bg_sigma = numpyro.sample(\"bg_sigma\", dist.HalfNormal(3.0))\n",
    "    bg_dist = dist.Normal(bg_mean, jnp.sqrt(bg_sigma**2 + yerr**2))\n",
    "\n",
    "    # Mixture\n",
    "    Q = numpyro.sample(\"Q\", dist.Uniform(0.0, 1.0))\n",
    "    mix = dist.Categorical(probs=jnp.array([Q, 1.0 - Q]))\n",
    "    numpyro.sample(\"obs\", MixtureGeneral(mix, [fg_dist, bg_dist]), obs=y)\n",
    "\n",
    "\n",
    "sampler = infer.MCMC(\n",
    "    infer.NUTS(linear_mixture_model),\n",
    "    num_warmup=2000,\n",
    "    num_samples=2000,\n",
    "    num_chains=2,\n",
    "    progress_bar=True,\n",
    ")\n",
    "%time sampler.run(jax.random.PRNGKey(10), x, yerr, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_data = az.from_numpyro(sampler)\n",
    "corner.corner(\n",
    "    inf_data,\n",
    "    var_names=[\"m\", \"b\", \"Q\"],\n",
    "    truths={\n",
    "        \"m\": true_params[0],\n",
    "        \"b\": true_params[1],\n",
    "        \"Q\": true_frac,\n",
    "    },\n",
    ")\n",
    "az.summary(inf_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.io import fits\n",
    "\n",
    "with fits.open(\"data/m67.fits.gz\") as f:\n",
    "    data = f[1].data\n",
    "\n",
    "mask = np.isfinite(data[\"parallax\"])\n",
    "mask &= np.isfinite(data[\"parallax_error\"])\n",
    "mask &= data[\"parallax\"] > 0.2\n",
    "mask &= data[\"parallax\"] < 3.0\n",
    "data = data[mask]\n",
    "\n",
    "plt.hist(data[\"parallax\"], 120, histtype=\"step\")\n",
    "plt.xlabel(\"parallax [mas]\")\n",
    "plt.ylabel(\"count\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A brief aside:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "L = 370.0\n",
    "x = np.linspace(0, 5000, 500)\n",
    "r = 0.5 * L * stats.chi2(df=6).rvs(500_000)\n",
    "plt.hist(r, 100, range=(0, 5000), density=True, histtype=\"step\", label=\"samples\")\n",
    "plt.plot(x, 0.5 * x**2 * np.exp(-x / L) / L**3, \"--\", label=\"pdf\")\n",
    "plt.xlabel(\"distance [pc]\")\n",
    "plt.yticks([])\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaia_single_model(plx_err, plx=None, plx_zp=0.0, L=370.0):\n",
    "    normed = numpyro.sample(\"normed\", dist.Chi2(6))\n",
    "    distance = numpyro.deterministic(\"distance\", 0.5 * L * normed)\n",
    "    numpyro.sample(\"plx\", dist.Normal(1000.0 / distance + plx_zp, plx_err), obs=plx)\n",
    "\n",
    "\n",
    "plx = data[1200][\"parallax\"]\n",
    "plx_err = data[1200][\"parallax_error\"]\n",
    "\n",
    "sampler = infer.MCMC(\n",
    "    infer.NUTS(gaia_single_model),\n",
    "    num_warmup=2000,\n",
    "    num_samples=2000,\n",
    "    num_chains=2,\n",
    ")\n",
    "%time sampler.run(jax.random.PRNGKey(0), plx_err, plx=plx)\n",
    "\n",
    "samples_single = sampler.get_samples()\n",
    "plt.hist(samples_single[\"distance\"], 50, density=True, histtype=\"step\")\n",
    "plt.xlabel(\"distance to target number 1200 [pc]\")\n",
    "plt.ylabel(\"posterior density\")\n",
    "plt.yticks([]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaia_cluster_model(plx_err, plx=None, plx_zp=0.0):\n",
    "    log_L = numpyro.sample(\"log_L\", dist.Normal(np.log(370.0), 2.0))\n",
    "    log_dist_clust = numpyro.sample(\"log_dist_clust\", dist.Normal(np.log(920.0), 2.0))\n",
    "    log_sigma = numpyro.sample(\"log_sigma\", dist.Normal(0.0, 2.0))\n",
    "    frac_clust = numpyro.sample(\"frac_clust\", dist.Uniform(0.0, 1.0))\n",
    "\n",
    "    L = numpyro.deterministic(\"L\", jnp.exp(log_L))\n",
    "    numpyro.deterministic(\"dist_clust\", jnp.exp(log_dist_clust))\n",
    "    numpyro.deterministic(\"approx_size_clust\", jnp.exp(log_sigma + log_dist_clust))\n",
    "\n",
    "    with numpyro.plate(\"stars\", len(plx_err)):\n",
    "\n",
    "        # The background distance distribution is the same as the one we used above,\n",
    "        # but this time we fit for the length scale L. Another difference is that we're\n",
    "        # \"transforming\" the distribution using an affine transformation, instead of\n",
    "        # sampling in the \"normalized distance\" and then multiplying by 0.5*L like we\n",
    "        # did above. These two approaches are equivalent, but we need to specify a\n",
    "        # single distribution here for use in the MixtureGeneral distribution below.\n",
    "        dist_bg = dist.TransformedDistribution(\n",
    "            dist.Chi2(6),\n",
    "            dist.transforms.AffineTransform(\n",
    "                0.0, 0.5 * L, domain=dist.constraints.positive\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # The foreground distribution is a Gaussian in log distance. Like with the\n",
    "        # background distribution, we use a transformation to convert from a Gaussian\n",
    "        # in log distance to a distribution in distance.\n",
    "        dist_fg = dist.TransformedDistribution(\n",
    "            dist.Normal(log_dist_clust, jnp.exp(log_sigma)),\n",
    "            dist.transforms.ExpTransform(),\n",
    "        )\n",
    "\n",
    "        # Now we \"mix\" the foreground and background distributions using the \"cluster\n",
    "        # membership fraction\" parameter to specify the mixing weights.\n",
    "        mixture = MixtureGeneral(\n",
    "            dist.Categorical(probs=jnp.stack((frac_clust, 1 - frac_clust), axis=-1)),\n",
    "            [dist_fg, dist_bg],\n",
    "        )\n",
    "        distance = numpyro.sample(\"distance\", mixture)\n",
    "\n",
    "        # Finally, we convert the distance to parallax and add the zero-point offset.\n",
    "        plx_true = numpyro.deterministic(\"plx_true\", 1000.0 / distance + plx_zp)\n",
    "        numpyro.sample(\"plx_obs\", dist.Normal(plx_true, plx_err), obs=plx)\n",
    "\n",
    "\n",
    "plx = np.ascontiguousarray(data[\"parallax\"], dtype=np.float32)\n",
    "plx_err = np.ascontiguousarray(data[\"parallax_error\"], dtype=np.float32)\n",
    "sampler = infer.MCMC(\n",
    "    infer.NUTS(gaia_cluster_model),\n",
    "    num_warmup=2000,\n",
    "    num_samples=4000,\n",
    "    num_chains=2,\n",
    "    progress_bar=True,\n",
    ")\n",
    "%time sampler.run(jax.random.PRNGKey(42), plx_err, plx=plx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = sampler.get_samples()\n",
    "plt.hist(\n",
    "    samples_single[\"distance\"],\n",
    "    50,\n",
    "    density=True,\n",
    "    histtype=\"step\",\n",
    "    label=\"single target model\",\n",
    ")\n",
    "plt.hist(\n",
    "    samples[\"distance\"][:, 1200],\n",
    "    50,\n",
    "    density=True,\n",
    "    histtype=\"step\",\n",
    "    label=\"cluster model\",\n",
    ")\n",
    "plt.xlabel(\"distance to target number 1200 [pc]\")\n",
    "plt.ylabel(\"posterior density\")\n",
    "plt.legend()\n",
    "plt.yticks([]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_data = az.from_numpyro(sampler)\n",
    "corner.corner(\n",
    "    inf_data,\n",
    "    var_names=[\"L\", \"dist_clust\", \"approx_size_clust\", \"frac_clust\"],\n",
    "    labels=[\n",
    "        \"background length scale [pc]\",\n",
    "        \"cluster distance [pc]\",\n",
    "        \"cluster intrinsic size [pc]\",\n",
    "        \"cluster membership fraction\",\n",
    "    ],\n",
    ")\n",
    "az.summary(inf_data, var_names=[\"L\", \"dist_clust\", \"approx_size_clust\", \"frac_clust\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = infer.Predictive(gaia_cluster_model, samples)(jax.random.PRNGKey(10), plx_err)\n",
    "\n",
    "_, bins, _ = plt.hist(plx, 50, histtype=\"step\", lw=2, label=\"observed\")\n",
    "label = \"posterior predictive\"\n",
    "for n in np.random.default_rng(0).integers(len(pred[\"plx_obs\"]), size=100):\n",
    "    plt.hist(\n",
    "        pred[\"plx_obs\"][n],\n",
    "        bins,\n",
    "        histtype=\"step\",\n",
    "        color=\"k\",\n",
    "        alpha=0.1,\n",
    "        lw=0.5,\n",
    "        label=label,\n",
    "    )\n",
    "    label = None\n",
    "plt.legend()\n",
    "plt.xlabel(\"mean parallax [mas]\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.xlim(bins[0], bins[-1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_pred = infer.Predictive(gaia_cluster_model, num_samples=50)(\n",
    "    jax.random.PRNGKey(11), plx_err\n",
    ")\n",
    "label = \"prior samples\"\n",
    "for n in range(len(prior_pred[\"plx_obs\"])):\n",
    "    plt.hist(\n",
    "        prior_pred[\"plx_obs\"][n],\n",
    "        100,\n",
    "        range=(0.0, 3.0),\n",
    "        histtype=\"step\",\n",
    "        color=\"k\",\n",
    "        lw=0.5,\n",
    "        alpha=0.5,\n",
    "        label=label,\n",
    "    )\n",
    "    label = None\n",
    "plt.legend()\n",
    "plt.xlabel(\"mean parallax [mas]\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.xlim(0, 3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b4c445ddd5eb40af76c64959af70f0a315c6821d655f4e1ee71e79b5a8cc47f9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
